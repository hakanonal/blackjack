{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI for BlackJack Game\n",
    "\n",
    "This notebook is for explorartion purposes. I keep this notebook as a journal.\n",
    "\n",
    "- This idea has been started with a conversation that I have made with my nephew. He asked me if we could build an AI agent that plays the best moves for blackjack. After some arguing on the topic, we have agreed that it would be overkill to train a deep net. Hence there were already best posible moves avaible. However this table only keeps the best action to play in a total matrix of player and dealer. So we have agreed to at least to construct a q-value table via reinforcement learning. We are expecting to construct a q-value table that will consists of the best move to to make (Hit,Double,Stand) in percentages. Our intension is to keep it simple so we will not implement the full rrules of the game. We will consider Ace as 1 not 11. I've learned that this is called hard totals. The avaible best moves are as follows:\n",
    "\n",
    "![Best Action Table](best_moves_in_hard_totals.jpeg)\n",
    "\n",
    "- So our plan is to put the percentages of each action for each state. (Q-Values)\n",
    "- We are also aware of [this](https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f) page. We may check it out in the future. But for learning purposes I will check it out if I geet stuck of my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.05.2020\n",
    "\n",
    "- I start right away coding the environment, I need to remember my other projects. First I will copy my abstract methods here.\n",
    "- So while checking my other [projects](https://github.com/hakanonal/geodashml) I need to decide the dimensions of the table. \n",
    "    - 1 dimension is for the playes's total card.\n",
    "    - 1 dimension is for the dealers open card.\n",
    "    - and one dimension is for the action. which is for simplicity Hit or Stand.\n",
    "- So the shape of the table is (21,10,2)\n",
    "- Ok very simple q-tabled agent has been completed.\n",
    "- action space 0 is stand and 1 is hit\n",
    "- Ok I am currentlly stuck at play method of environment class. I need to think thourgh how should construct reward. Since getting the hihgest number would not be promoted. But I need to give reward to get more that dealer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.05.2020\n",
    "\n",
    "- I have started to read [this](https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f)\n",
    "- I have completed reding. My First note is that in conclusion the wins and lose counts are almost same. I need to underrstand the reward mecanism better to make comments on this. \n",
    "- Writer sugested 3 addtional improvements on his code which I bealive I can apply fitst two on them. 1-do more episodes, 2-try different learning and exploration rate.\n",
    "- Writer implemented the ace or 1 succesfully. I will notte that for later use.\n",
    "- I have also relized that writer structured tthe q-table as 2 dim array and first dim keeps tuple as index, which is very convinient to easily expand the features of the state paramters. So I have changes it\n",
    "- In terrms of reward, this example did not updated the q-table in every action but collects all the actions in a list forr every 1 episode and then loops though backwards to update. Which I think I am going to use the same tactic here. \n",
    "- I will continue later to adapt to play actions till end of the game and calculate and update the reward afterwards for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01.06.2020\n",
    "\n",
    "- I have continues on developing agent and environment\n",
    "- I have also tested my first run and startd to correct the syntax and other types of error.\n",
    "- I have seen that if I convert the q-table numpy array to dictionary I understant that I need to change the state into tuple.\n",
    "- However tuple values can not be updated and state must be updated all the time. \n",
    "- So I have converted the state also to dictionary.\n",
    "- It seems that I am on correct path. Now since I have converted q-table to dictionary I need to init all possible values. otherwise I get the key error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "class agent:\n",
    "\n",
    "    def __init__(self,discount,exploration_rate,decay_factor, learning_rate):\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.decay_factor = decay_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return np.argmax(self.q_table[(state['player_sum'],state['dealer_sum'])])\n",
    "    def random_action(self):\n",
    "        return random.random() > 0.5\n",
    "\n",
    "\n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        \n",
    "        old_state_prediction = self.q_table[(old_state['player_sum'],old_state['dealer_sum'])][action]\n",
    "        new_state_prediction = self.q_table[(new_state['player_sum'],new_state['dealer_sum'])]\n",
    "\n",
    "        old_state_prediction = ((1-self.learning_rate) * old_state_prediction) + (self.learning_rate * (reward + self.discount * np.amax(new_state_prediction)))\n",
    "\n",
    "        self.q_table[(old_state['player_sum'],old_state['dealer_sum'])][action] = old_state_prediction\n",
    "        return old_state_prediction\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):        \n",
    "        reward = self.train(old_state, new_state, action, reward)\n",
    "        self.exploration_rate *= self.decay_factor\n",
    "        return reward\n",
    "\n",
    "    def save(self, file=\"policy\"):\n",
    "        fw = open(file, 'wb')\n",
    "        pickle.dump(self.q_table, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load(self, file=\"policy\"):\n",
    "        fr = open(file, 'rb')\n",
    "        self.q_table = pickle.load(fr)\n",
    "        fr.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class environment:\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        self.config = config\n",
    "        self.agent = agent(\n",
    "            discount=self.config['discount'],\n",
    "            exploration_rate=self.config['exploration_rate'],\n",
    "            decay_factor=self.config['decay_factor'],\n",
    "            learning_rate=self.config['learning_rate']\n",
    "            )\n",
    "        self.initGame()\n",
    "\n",
    "    def initGame(self):\n",
    "        self.state = {'player_sum':0,'dealer_sum':0} \n",
    "        self.state['player_sum'] += self.hit()\n",
    "        self.state['player_sum'] += self.hit()\n",
    "        self.state['dealer_sum'] += self.hit()\n",
    "        self.actions_played = []\n",
    "\n",
    "    def start(self):\n",
    "        for episode in range(1,self.config['episode']+1):\n",
    "            self.initGame()\n",
    "            #player turn\n",
    "            while True:\n",
    "                action_to_play = self.agent.get_next_action(self.state)\n",
    "                new_state, ended = self.play(action_to_play)\n",
    "                self.actions_played.append((self.state,new_state,action_to_play))\n",
    "                self.state = new_state\n",
    "                if ended:\n",
    "                    break\n",
    "\n",
    "            #dealer turn\n",
    "            while True:\n",
    "                new_state, ended = self.playDealer()\n",
    "                self.state = new_state\n",
    "                if ended:\n",
    "                    break\n",
    "\n",
    "            #q-tatble update backpropogation\n",
    "            reward = self.findWinner()\n",
    "            for old_state,new_state,action in reversed(self.actions_played):\n",
    "                reward = self.agent.update(self.state,new_state,action,reward)\n",
    "\n",
    "    def hit(self):\n",
    "        return random.randint(1,10)\n",
    "\n",
    "    def play(self,action):\n",
    "        new_state = self.state.copy()\n",
    "        ended = False\n",
    "        if action:\n",
    "            new_state['player_sum'] += self.hit()\n",
    "        else:\n",
    "            ended = True\n",
    "        if new_state['player_sum'] > 21:\n",
    "            ended = True\n",
    "        return new_state, ended\n",
    "        \n",
    "    def playDealer(self):\n",
    "        new_state = self.state.copy()\n",
    "        ended = False\n",
    "        new_state['dealer_sum'] += self.hit()\n",
    "        if new_state['dealer_sum'] >= 17:\n",
    "            ended = True\n",
    "        return new_state, ended\n",
    "        \n",
    "    def findWinner(self):\n",
    "        # player 1 | draw 0 | dealer -1\n",
    "        winner = 0\n",
    "        if self.state['player_sum'] > 21:\n",
    "            if self.state['dealer_sum'] > 21:\n",
    "                winner = 0\n",
    "            else:\n",
    "                winner = -1\n",
    "        else:\n",
    "            if self.state['dealer_sum'] > 21:\n",
    "                winner = 1\n",
    "            else:\n",
    "                if self.state['player_sum'] < self.state['dealer_sum']:\n",
    "                    winner = -1\n",
    "                elif self.state['player_sum'] > self.state['dealer_sum']:\n",
    "                    winner = 1\n",
    "                else:\n",
    "                    winner = 0\n",
    "        return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "(4, 22)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ed20cd40c3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-e97fe31935b5>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindWinner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_played\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-aa1170eafb92>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, old_state, new_state, action, reward)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-aa1170eafb92>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, old_state, new_state, action, reward)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mold_state_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dealer_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mnew_state_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dealer_sum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (4, 22)"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'discount': 0.95,\n",
    "    'exploration_rate':0.5,\n",
    "    'decay_factor':0.999,\n",
    "    'learning_rate':1,\n",
    "    'episode':1000\n",
    "}\n",
    "\n",
    "e = environment(config=config)\n",
    "e.start()\n",
    "e.agent.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bit3ef340d3d96f44f381aa898ad19adcf0",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}