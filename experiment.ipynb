{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI for BlackJack Game\n",
    "\n",
    "This notebook is for explorartion purposes. I keep this notebook as a journal.\n",
    "\n",
    "- This idea has been started with a conversation that I have made with my nephew. He asked me if we could build an AI agent that plays the best moves for blackjack. After some arguing on the topic, we have agreed that it would be overkill to train a deep net. Hence there were already best posible moves avaible. However this table only keeps the best action to play in a total matrix of player and dealer. So we have agreed to at least to construct a q-value table via reinforcement learning. We are expecting to construct a q-value table that will consists of the best move to to make (Hit,Double,Stand) in percentages. Our intension is to keep it simple so we will not implement the full rrules of the game. We will consider Ace as 1 not 11. I've learned that this is called hard totals. The avaible best moves are as follows:\n",
    "\n",
    "![Best Action Table](best_moves_in_hard_totals.jpeg)\n",
    "\n",
    "- So our plan is to put the percentages of each action for each state. (Q-Values)\n",
    "- We are also aware of [this](https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f) page. We may check it out in the future. But for learning purposes I will check it out if I geet stuck of my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.05.2020\n",
    "\n",
    "- I start right away coding the environment, I need to remember my other projects. First I will copy my abstract methods here.\n",
    "- So while checking my other [projects](https://github.com/hakanonal/geodashml) I need to decide the dimensions of the table. \n",
    "    - 1 dimension is for the playes's total card.\n",
    "    - 1 dimension is for the dealers open card.\n",
    "    - and one dimension is for the action. which is for simplicity Hit or Stand.\n",
    "- So the shape of the table is (21,10,2)\n",
    "- Ok very simple q-tabled agent has been completed.\n",
    "- action space 0 is stand and 1 is hit\n",
    "- Ok I am currentlly stuck at play method of environment class. I need to think thourgh how should construct reward. Since getting the hihgest number would not be promoted. But I need to give reward to get more that dealer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.05.2020\n",
    "\n",
    "- I have started to read [this](https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f)\n",
    "- I have completed reding. My First note is that in conclusion the wins and lose counts are almost same. I need to underrstand the reward mecanism better to make comments on this. \n",
    "- Writer sugested 3 addtional improvements on his code which I bealive I can apply fitst two on them. 1-do more episodes, 2-try different learning and exploration rate.\n",
    "- Writer implemented the ace or 1 succesfully. I will notte that for later use.\n",
    "- I have also relized that writer structured tthe q-table as 2 dim array and first dim keeps tuple as index, which is very convinient to easily expand the features of the state paramters. So I have changes it\n",
    "- In terrms of reward, this example did not updated the q-table in every action but collects all the actions in a list forr every 1 episode and then loops though backwards to update. Which I think I am going to use the same tactic here. \n",
    "- I will continue later to adapt to play actions till end of the game and calculate and update the reward afterwards for each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01.06.2020\n",
    "\n",
    "- I have continues on developing agent and environment\n",
    "- I have also tested my first run and startd to correct the syntax and other types of error.\n",
    "- I have seen that if I convert the q-table numpy array to dictionary I understant that I need to change the state into tuple.\n",
    "- However tuple values can not be updated and state must be updated all the time. \n",
    "- So I have converted the state also to dictionary.\n",
    "- It seems that I am on correct path. Now since I have converted q-table to dictionary I need to init all possible values. otherwise I get the key error.\n",
    "- I have implemented a getq function to check if the key is exists when accesing the q-value, if there is no q value I initilize as 0.\n",
    "- Dude it is unbeliavable fast did I somtething wrong?\n",
    "- I am turning the beast on, trying different config parameters.\n",
    "- However, my intension is to detaili debug if the logic is working correctlly.\n",
    "- I have seen in the article that the q-table has been stored the file using the pickle module. So I also used that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "class agent:\n",
    "\n",
    "    def __init__(self,discount,exploration_rate,decay_factor, learning_rate):\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.decay_factor = decay_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return np.argmax(self.getQ(state))\n",
    "    def random_action(self):\n",
    "        return random.random() > 0.5\n",
    "\n",
    "    def getQ(self,state):\n",
    "        player_sum = state['player_sum']\n",
    "        dealer_sum = state['dealer_sum']\n",
    "        if (player_sum,dealer_sum) not in self.q_table:\n",
    "            self.q_table[(player_sum,dealer_sum)] = [0,0]\n",
    "        return self.q_table[(player_sum,dealer_sum)]\n",
    "\n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        \n",
    "        old_state_prediction = self.getQ(old_state)[action]\n",
    "        new_state_prediction = self.getQ(new_state)\n",
    "\n",
    "        old_state_prediction = ((1-self.learning_rate) * old_state_prediction) + (self.learning_rate * (reward + self.discount * np.amax(new_state_prediction)))\n",
    "\n",
    "        self.q_table[(old_state['player_sum'],old_state['dealer_sum'])][action] = old_state_prediction\n",
    "        return old_state_prediction\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):        \n",
    "        reward = self.train(old_state, new_state, action, reward)\n",
    "        self.exploration_rate *= self.decay_factor\n",
    "        return reward\n",
    "\n",
    "    def save(self, file=\"policy\"):\n",
    "        fw = open(file, 'wb')\n",
    "        pickle.dump(self.q_table, fw)\n",
    "        fw.close()\n",
    "\n",
    "    def load(self, file=\"policy\"):\n",
    "        fr = open(file, 'rb')\n",
    "        self.q_table = pickle.load(fr)\n",
    "        fr.close()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class environment:\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        self.config = config\n",
    "        self.agent = agent(\n",
    "            discount=self.config['discount'],\n",
    "            exploration_rate=self.config['exploration_rate'],\n",
    "            decay_factor=self.config['decay_factor'],\n",
    "            learning_rate=self.config['learning_rate']\n",
    "            )\n",
    "        self.initGame()\n",
    "        self.metrics = {\n",
    "            'tot_win' : 0,\n",
    "            'tot_draw' : 0,\n",
    "            'tot_lose': 0,\n",
    "        }\n",
    "\n",
    "    def initGame(self):\n",
    "        self.state = {'player_sum':0,'dealer_sum':0} \n",
    "        self.state['player_sum'] += self.hit()\n",
    "        self.state['player_sum'] += self.hit()\n",
    "        self.state['dealer_sum'] += self.hit()\n",
    "        self.actions_played = []\n",
    "\n",
    "    def start(self):\n",
    "        for episode in range(1,self.config['episode']+1):\n",
    "            self.initGame()\n",
    "            #player turn\n",
    "            while True:\n",
    "                action_to_play = self.agent.get_next_action(self.state)\n",
    "                new_state, ended = self.play(action_to_play)\n",
    "                self.actions_played.append((self.state,new_state,action_to_play))\n",
    "                self.debug1(episode,self.state,new_state,action_to_play)\n",
    "                self.state = new_state\n",
    "                if ended:\n",
    "                    break\n",
    "\n",
    "            #dealer turn\n",
    "            while True:\n",
    "                new_state, ended = self.playDealer()\n",
    "                self.debug1(episode,self.state,new_state,-1)\n",
    "                self.state = new_state\n",
    "                if ended:\n",
    "                    break\n",
    "\n",
    "            #q-tatble update backpropogation\n",
    "            reward = self.findWinner()\n",
    "            if reward == 1:\n",
    "                self.metrics['tot_win'] += 1\n",
    "                self.metrics['avg_win'] = self.metrics['tot_win'] / episode\n",
    "            if reward == 0:\n",
    "                self.metrics['tot_draw'] += 1\n",
    "                self.metrics['avg_draw'] = self.metrics['tot_draw'] / episode\n",
    "            if reward == -1:\n",
    "                self.metrics['tot_lose'] += 1\n",
    "                self.metrics['avg_lose'] = self.metrics['tot_lose'] / episode\n",
    "            for old_state,new_state,action in reversed(self.actions_played):\n",
    "                new_reward = self.agent.update(old_state,new_state,action,reward)\n",
    "                self.debug2(episode,old_state,new_state,action,reward,new_reward)\n",
    "                reward = new_reward\n",
    "\n",
    "    def hit(self):\n",
    "        return random.randint(1,10)\n",
    "\n",
    "    def play(self,action):\n",
    "        new_state = self.state.copy()\n",
    "        ended = False\n",
    "        if action:\n",
    "            new_state['player_sum'] += self.hit()\n",
    "        else:\n",
    "            ended = True\n",
    "        if new_state['player_sum'] > 21:\n",
    "            ended = True\n",
    "        return new_state, ended\n",
    "        \n",
    "    def playDealer(self):\n",
    "        new_state = self.state.copy()\n",
    "        ended = False\n",
    "        new_state['dealer_sum'] += self.hit()\n",
    "        if new_state['dealer_sum'] >= 17:\n",
    "            ended = True\n",
    "        return new_state, ended\n",
    "        \n",
    "    def findWinner(self):\n",
    "        # player 1 | draw 0 | dealer -1\n",
    "        winner = 0\n",
    "        if self.state['player_sum'] > 21:\n",
    "            if self.state['dealer_sum'] > 21:\n",
    "                winner = 0\n",
    "            else:\n",
    "                winner = -1\n",
    "        else:\n",
    "            if self.state['dealer_sum'] > 21:\n",
    "                winner = 1\n",
    "            else:\n",
    "                if self.state['player_sum'] < self.state['dealer_sum']:\n",
    "                    winner = -1\n",
    "                elif self.state['player_sum'] > self.state['dealer_sum']:\n",
    "                    winner = 1\n",
    "                else:\n",
    "                    winner = 0\n",
    "        return winner\n",
    "    \n",
    "    def debug1(self,episode,old_state,new_state,action):\n",
    "        if(self.config['debug']):\n",
    "            print(\"%d = %s -> %s -> %s\"%(episode,old_state,action,new_state))\n",
    "            input(\"continue?\")\n",
    "\n",
    "    def debug2(self,episode,old_state,new_state,action,old_reward,new_reward):\n",
    "        if(self.config['debug']):\n",
    "            print(\"%d = %s -> %s -> %s | %s->%s\"%(episode,old_state,action,new_state,old_reward,new_reward))\n",
    "            input(\"continue?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'discount': 0.5,\n",
    "    'exploration_rate':0.5,\n",
    "    'decay_factor':0.9999,\n",
    "    'learning_rate':0.0001,\n",
    "    'episode':1000000,\n",
    "    'debug' : 0,\n",
    "}\n",
    "\n",
    "e = environment(config=config)\n",
    "#e.start()\n",
    "#e.agent.save()\n",
    "#e.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01.06.2020\n",
    "\n",
    "- let's plot tthe q-table and see it is going to be mean something. To do that I will add a method into agent class that converts the q-table dictionary to multi dim numpy array.\n",
    "- Well I have converted the qtable into numpy array. I have omitted the values that is not being represented for valid total. Suprasinglly all values are zero. I need to debug better if rewarrding mechanism is working good.\n",
    "- I have added a debug method to see turns plays and steps are working fine. it seems ok. I will heck reward calcıulation also.\n",
    "- Ok I think I have found the problem, in backpropogation in every update instead of using the old_state I have always used the self.state whichnis the last state of the game which is wrong.\n",
    "- When I trained this way I have increase the win rate tto 40%, On the other hand draw position is also increased\n",
    "- When I plot the q-values I have a very non-meaningful graph. I need to fins a better way to present this data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.agent.load()\n",
    "e.agent.q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "Z = [] \n",
    "for state,alist in e.agent.q_table.items():\n",
    "    if(state[0] > 21):\n",
    "        continue\n",
    "    if(state[1] > 10):\n",
    "        continue\n",
    "    X.append([state[0],alist[0]])\n",
    "    X.append([state[0],alist[1]])\n",
    "    Y.append([state[1],alist[0]])\n",
    "    Y.append([state[1],alist[1]])\n",
    "    Z.append([0,alist[0]])\n",
    "    Z.append([1,alist[1]])\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Z = np.array(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(X,Y,Z,cmap=\"Oranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02.06.2020\n",
    "\n",
    "- I want to represent q-table nicelly. So today I am going to work on that. \n",
    "- To do that I am planing to create a different notebook just reads the policy file and shows the values nicelly. I can document it good so that everrybody can read and understand that document.\n",
    "- I am leaving the above method just as is but I did not like it.\n",
    "- Suprisinglly I did not find any convienient library to acomplish this. Best documented view is the matpltlib's one [here](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.table.html)\n",
    "- I will give try some [pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) dataframe\n",
    "- yeah it did nott work the error message = \"Must pass 2-d input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "e.agent.load()\n",
    "e.agent.q_table\n",
    "\n",
    "d = np.zeros((22,11,2))\n",
    "for state,alist in e.agent.q_table.items():\n",
    "    if(state[0] > 21):\n",
    "        continue\n",
    "    if(state[1] > 10):\n",
    "        continue\n",
    "    d[state[0]][state[1]][0] = alist[0]\n",
    "    d[state[0]][state[1]][1] = alist[1]\n",
    "\n",
    "df = pandas.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ok! I will also check wandb if there is any table view tools.\n",
    "- there is a ttabşe view option documented well [here](https://docs.wandb.com/library/log#logging-text-tables). Let's give it a try!\n",
    "- I did not understand but I got an error \"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\" and I do not see how to fix. I also could not find any feedback reagrding this error related to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "e.agent.load()\n",
    "e.agent.q_table\n",
    "\n",
    "d = np.zeros((22,11))\n",
    "for state,alist in e.agent.q_table.items():\n",
    "    if(state[0] > 21):\n",
    "        continue\n",
    "    if(state[1] > 10):\n",
    "        continue\n",
    "    #d[state[0]][state[1]][0] = alist[0]\n",
    "    #d[state[0]][state[1]][1] = alist[1]\n",
    "    d[state[0]][state[1]] = alist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"%d\"%i for i in range(11)]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t = wandb.Table(data=d,columns=[\"%d\"%i for i in range(11)])\n",
    "wandb.log({\"test-table\":t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My last bullet is to manually nivelly print the 3d table. Since the table is fixed size it will not be complicated I prosume.\n",
    "- I have comleted the widelly all table below.\n",
    "- I am having problem with the negative value where negattive values has minus sign before the number which results to mis align the table.\n",
    "- So I have checked to colorize the values and print all values as absolute value. [This](https://github.com/welbornprod/colr) page is very helpful\n",
    "- I have prefered to use [this](https://i.stack.imgur.com/6otvY.png)\n",
    "- Alright so simply for every column there is two values: first one is forr stand and the second one is for hit. positive value are green and negative one are red. the q-values are gives the likelihood to win or loose if you take that specific action.\n",
    "- So I will put this code to another notebook. \n",
    "- Wait why we do not have any values on firrst row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-------------------------------------------------------------------------------------------------------\n  |    1    |    2    |    3    |    4    |    5    |    6    |    7    |    8    |    9    |   10    |\n01|\n02|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n03|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n04|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\n05|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\n06|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\n07|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.04\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\n08|\u001b[1;31;40m0.04\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.04\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.05\u001b[0m|\u001b[6;31;47m0.05\u001b[0m|\n09|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.01\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;32;47m0.01\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.04\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\n10|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.02\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.02\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.02\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.03\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.03\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.03\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.05\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.06\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.04\u001b[0m|\u001b[1;31;40m0.01\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n11|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.07\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.08\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.06\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.07\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.08\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.09\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.11\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.11\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.09\u001b[0m|\u001b[1;31;40m0.00\u001b[0m|\u001b[6;32;47m0.05\u001b[0m|\n12|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.03\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.02\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;32;47m0.01\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.02\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.03\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.05\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.05\u001b[0m|\u001b[1;31;40m0.02\u001b[0m|\u001b[6;32;47m0.04\u001b[0m|\u001b[1;31;40m0.03\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n13|\u001b[1;31;40m0.06\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\u001b[1;31;40m0.06\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\u001b[1;31;40m0.06\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\u001b[1;31;40m0.06\u001b[0m|\u001b[6;31;47m0.03\u001b[0m|\u001b[1;31;40m0.05\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.05\u001b[0m|\u001b[6;31;47m0.01\u001b[0m|\u001b[1;31;40m0.05\u001b[0m|\u001b[6;32;47m0.00\u001b[0m|\u001b[1;31;40m0.05\u001b[0m|\u001b[6;32;47m0.00\u001b[0m|\u001b[1;31;40m0.06\u001b[0m|\u001b[6;31;47m0.02\u001b[0m|\u001b[1;31;40m0.07\u001b[0m|\u001b[6;31;47m0.05\u001b[0m|\n14|\u001b[1;31;40m0.09\u001b[0m|\u001b[6;31;47m0.09\u001b[0m|\u001b[1;31;40m0.09\u001b[0m|\u001b[6;31;47m0.08\u001b[0m|\u001b[1;31;40m0.10\u001b[0m|\u001b[6;31;47m0.09\u001b[0m|\u001b[1;31;40m0.09\u001b[0m|\u001b[6;31;47m0.08\u001b[0m|\u001b[1;31;40m0.08\u001b[0m|\u001b[6;31;47m0.07\u001b[0m|\u001b[1;31;40m0.08\u001b[0m|\u001b[6;31;47m0.07\u001b[0m|\u001b[1;31;40m0.08\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\u001b[1;31;40m0.09\u001b[0m|\u001b[6;31;47m0.06\u001b[0m|\u001b[1;31;40m0.09\u001b[0m|\u001b[6;31;47m0.07\u001b[0m|\u001b[1;31;40m0.12\u001b[0m|\u001b[6;31;47m0.11\u001b[0m|\n15|\u001b[1;31;40m0.13\u001b[0m|\u001b[6;31;47m0.13\u001b[0m|\u001b[1;31;40m0.13\u001b[0m|\u001b[6;31;47m0.12\u001b[0m|\u001b[1;31;40m0.13\u001b[0m|\u001b[6;31;47m0.13\u001b[0m|\u001b[1;31;40m0.13\u001b[0m|\u001b[6;31;47m0.13\u001b[0m|\u001b[1;31;40m0.12\u001b[0m|\u001b[6;31;47m0.11\u001b[0m|\u001b[1;31;40m0.11\u001b[0m|\u001b[6;31;47m0.10\u001b[0m|\u001b[1;31;40m0.11\u001b[0m|\u001b[6;31;47m0.09\u001b[0m|\u001b[1;31;40m0.12\u001b[0m|\u001b[6;31;47m0.10\u001b[0m|\u001b[1;31;40m0.13\u001b[0m|\u001b[6;31;47m0.12\u001b[0m|\u001b[1;31;40m0.15\u001b[0m|\u001b[6;31;47m0.15\u001b[0m|\n16|\u001b[1;31;40m0.17\u001b[0m|\u001b[6;31;47m0.16\u001b[0m|\u001b[1;31;40m0.16\u001b[0m|\u001b[6;31;47m0.16\u001b[0m|\u001b[1;31;40m0.16\u001b[0m|\u001b[6;31;47m0.15\u001b[0m|\u001b[1;31;40m0.15\u001b[0m|\u001b[6;31;47m0.14\u001b[0m|\u001b[1;31;40m0.15\u001b[0m|\u001b[6;31;47m0.14\u001b[0m|\u001b[1;31;40m0.14\u001b[0m|\u001b[6;31;47m0.13\u001b[0m|\u001b[1;31;40m0.14\u001b[0m|\u001b[6;31;47m0.12\u001b[0m|\u001b[1;31;40m0.14\u001b[0m|\u001b[6;31;47m0.13\u001b[0m|\u001b[1;31;40m0.16\u001b[0m|\u001b[6;31;47m0.15\u001b[0m|\u001b[1;31;40m0.18\u001b[0m|\u001b[6;31;47m0.18\u001b[0m|\n17|\u001b[1;31;40m0.17\u001b[0m|\u001b[6;31;47m0.17\u001b[0m|\u001b[1;31;40m0.17\u001b[0m|\u001b[6;31;47m0.17\u001b[0m|\u001b[1;31;40m0.17\u001b[0m|\u001b[6;31;47m0.17\u001b[0m|\u001b[1;31;40m0.16\u001b[0m|\u001b[6;31;47m0.16\u001b[0m|\u001b[1;31;40m0.15\u001b[0m|\u001b[6;31;47m0.15\u001b[0m|\u001b[1;31;40m0.14\u001b[0m|\u001b[6;31;47m0.14\u001b[0m|\u001b[1;31;40m0.13\u001b[0m|\u001b[6;31;47m0.13\u001b[0m|\u001b[1;31;40m0.15\u001b[0m|\u001b[6;31;47m0.15\u001b[0m|\u001b[1;31;40m0.18\u001b[0m|\u001b[6;31;47m0.18\u001b[0m|\u001b[1;31;40m0.20\u001b[0m|\u001b[6;31;47m0.20\u001b[0m|\n18|\u001b[1;32;40m0.06\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.08\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.04\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.06\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.09\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.10\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.20\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.18\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.05\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;31;40m0.04\u001b[0m|\u001b[6;31;47m0.04\u001b[0m|\n19|\u001b[1;32;40m0.38\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.38\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.37\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.37\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.37\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.40\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.45\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.50\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.46\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.31\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n20|\u001b[1;32;40m0.60\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.59\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.58\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.58\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.59\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.58\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.62\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.65\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.69\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.62\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n21|\u001b[1;32;40m0.69\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.71\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.69\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.68\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.68\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.68\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.71\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.73\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.74\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\u001b[1;32;40m0.72\u001b[0m|\u001b[6;31;47m0.00\u001b[0m|\n-------------------------------------------------------------------------------------------------------\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "e.agent.load()\n",
    "e.agent.q_table\n",
    "_sign = lambda x: x and (1, -1)[x<0]\n",
    "\n",
    "d = np.zeros((22,11,2))\n",
    "for state,alist in e.agent.q_table.items():\n",
    "    if(state[0] > 21):\n",
    "        continue\n",
    "    if(state[1] > 10):\n",
    "        continue\n",
    "    d[state[0]][state[1]][0] = alist[0]\n",
    "    d[state[0]][state[1]][1] = alist[1]\n",
    "\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"  |    1    |    2    |    3    |    4    |    5    |    6    |    7    |    8    |    9    |   10    |\")\n",
    "for i in range(1,22):\n",
    "    print(\"%02d|\"%i, end=\"\")\n",
    "    for j in range(1,11):\n",
    "        if _sign(d[i][j][0]) == 1:\n",
    "            print(\"\\x1b[1;32;40m%0.2f\\x1b[0m\"%d[i][j][0],end=\"|\")\n",
    "        if _sign(d[i][j][0]) == -1:\n",
    "            print(\"\\x1b[1;31;40m%0.2f\\x1b[0m\"%abs(d[i][j][0]),end=\"|\")\n",
    "        if _sign(d[i][j][1]) == 1:\n",
    "            print(\"\\x1b[6;32;47m%0.2f\\x1b[0m\"%d[i][j][1],end=\"|\")\n",
    "        if _sign(d[i][j][1]) == -1:\n",
    "            print(\"\\x1b[6;31;47m%0.2f\\x1b[0m\"%abs(d[i][j][1]),end=\"|\")\n",
    "    print(\"\")\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bit3ef340d3d96f44f381aa898ad19adcf0",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}