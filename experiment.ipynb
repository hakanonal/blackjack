{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building AI for BlackJack Game\n",
    "\n",
    "This notebook is for explorartion purposes. I keep this notebook as a journal.\n",
    "\n",
    "- This idea has been started with a conversation that I have made with my nephew. He asked me if we could build an AI agent that plays the best moves for blackjack. After some arguing on the topic, we have agreed that it would be overkill to train a deep net. Hence there were already best posible moves avaible. However this table only keeps the best action to play in a total matrix of player and dealer. So we have agreed to at least to construct a q-value table via reinforcement learning. We are expecting to construct a q-value table that will consists of the best move to to make (Hit,Double,Stand) in percentages. Our intension is to keep it simple so we will not implement the full rrules of the game. We will consider Ace as 1 not 11. I've learned that this is called hard totals. The avaible best moves are as follows:\n",
    "\n",
    "![Best Action Table](best_moves_in_hard_totals.jpeg)\n",
    "\n",
    "- So our plan is to put the percentages of each action for each state. (Q-Values)\n",
    "- We are also aware of [this](https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f) page. We may check it out in the future. But for learning purposes I will check it out if I geet stuck of my own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.05.2020\n",
    "\n",
    "- I start right away coding the environment, I need to remember my other projects. First I will copy my abstract methods here.\n",
    "- So while checking my other [projects](https://github.com/hakanonal/geodashml) I need to decide the dimensions of the table. \n",
    "    - 1 dimension is for the playes's total card.\n",
    "    - 1 dimension is for the dealers open card.\n",
    "    - and one dimension is for the action. which is for simplicity Hit or Stand.\n",
    "- So the shape of the table is (21,10,2)\n",
    "- Ok very simple q-tabled agent has been completed.\n",
    "- action space 0 is stand and 1 is hit\n",
    "- Ok I am currentlly stuck at play method of environment class. I need to think thourgh how should construct reward. Since getting the hihgest number hould not be promoted. But I need to give reward to get more that dealer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class agent:\n",
    "\n",
    "    def __init__(self,discount=0.95,exploration_rate=0.9,decay_factor=0.9999, learning_rate=0.1):\n",
    "        self.discount = discount # How much we appreciate future reward over current\n",
    "        self.exploration_rate = exploration_rate # Initial exploration rate\n",
    "        self.decay_factor = decay_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.q_table = np.zeros((21,10,2))\n",
    "        \n",
    "    def get_next_action(self, state):\n",
    "        if random.random() < self.exploration_rate: # Explore (gamble) or exploit (greedy)\n",
    "            return self.random_action()\n",
    "        else:\n",
    "            return self.greedy_action(state)\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        return np.argmax(self.q_table[state[0]][state[1]])\n",
    "    def random_action(self):\n",
    "        return random.random() > 0.5\n",
    "\n",
    "\n",
    "    def train(self, old_state, new_state, action, reward):\n",
    "        \n",
    "        old_state_prediction = self.q_table[old_state[0]][old_state[1]][action]\n",
    "        new_state_prediction = self.q_table[new_state[0]][new_state[1]]\n",
    "\n",
    "        old_state_prediction = ((1-self.learning_rate) * old_state_prediction) + (self.learning_rate * (reward + self.discount * np.amax(new_state_prediction)))\n",
    "\n",
    "        self.q_table[old_state[0]][old_state[1]][action] = old_state_prediction\n",
    "\n",
    "    def update(self, old_state, new_state, action, reward):        \n",
    "        self.train(old_state, new_state, action, reward)\n",
    "        self.exploration_rate *= self.decay_factor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class environment:\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        self.config = config\n",
    "        self.agent = agent(\n",
    "            discount=self.config['discount'],\n",
    "            exploration_rate=self.config['exploration_rate'],\n",
    "            decay_factor=self.config['decay_factor'],\n",
    "            learning_rate=self.config['learning_rate']\n",
    "            )\n",
    "        self.initGame()\n",
    "\n",
    "    def initGame(self):\n",
    "        self.state = (0,0) #a tuble first is players total, second is dealers open card.\n",
    "        self.state[0] += self.hit()\n",
    "        self.state[0] += self.hit()\n",
    "        self.state[1] += self.hit()\n",
    "\n",
    "    def start(self):\n",
    "        for episode in range(1,self.config['episode']+1):\n",
    "            self.initGame()\n",
    "            while True:\n",
    "                action_to_play = self.agent.get_next_action(self.state)\n",
    "                new_state, reward = self.play(action_to_play)\n",
    "                self.agent.update(self.state,new_state,action_to_play,reward)\n",
    "\n",
    "    def hit(self):\n",
    "        return random.randint(1,10)\n",
    "\n",
    "    def play(self,action):\n",
    "        new_state = (self.state[0],self.state[1])\n",
    "        reward = 0\n",
    "        if action:\n",
    "            new_state[0] += self.hit()\n",
    "        if new_state[0] > 21:\n",
    "            reward = -10\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37264bit3ef340d3d96f44f381aa898ad19adcf0",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}